import os
import logging
import sqlite3
from datetime import datetime, timedelta
import numpy as np
import uvicorn
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

import tensorflow as tf
import tensorflow_hub as hub
import librosa
import tempfile
import pytz
import joblib

# ---- Logging ----
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("cough_server")

app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ---- Configuration ----
SERVER_TIMEZONE = pytz.timezone('Europe/Moscow')
CLEANUP_INTERVAL_HOURS = 1  # –£–¥–∞–ª—è—Ç—å –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞
THRESHOLD = 0.5  # –ü–æ—Ä–æ–≥ –¥–ª—è –∫–∞—à–ª—è (–º–æ–∂–µ—à—å –ø–æ–º–µ–Ω—è—Ç—å)

def get_current_datetime():
    return datetime.now(SERVER_TIMEZONE).strftime("%Y-%m-%d %H:%M:%S")

def get_current_date():
    return datetime.now(SERVER_TIMEZONE).strftime("%Y-%m-%d")

# ---- Database ----
DB_PATH = "cough_db.db"

def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–æ–π"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS cough_records (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            device_id TEXT,
            filename TEXT,
            file_path TEXT,
            probability REAL,
            cough_detected INTEGER,
            message TEXT,
            top_classes TEXT,
            cough_stats TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_device_time ON cough_records(device_id, timestamp)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_cough_detected ON cough_records(cough_detected)')
    
    conn.commit()
    conn.close()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    cleanup_old_records()
    logger.info("‚úÖ Database initialized with cleanup")

def cleanup_old_records():
    """–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π —Å—Ç–∞—Ä—à–µ CLEANUP_INTERVAL_HOURS —á–∞—Å–æ–≤"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cutoff_time = (datetime.now(SERVER_TIMEZONE) - 
                      timedelta(hours=CLEANUP_INTERVAL_HOURS)).strftime("%Y-%m-%d %H:%M:%S")
        
        cursor.execute('''
            DELETE FROM cough_records 
            WHERE timestamp < ?
        ''', (cutoff_time,))
        
        deleted_count = cursor.rowcount
        conn.commit()
        conn.close()
        
        if deleted_count > 0:
            logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {deleted_count} –∑–∞–ø–∏—Å–µ–π —Å—Ç–∞—Ä—à–µ {CLEANUP_INTERVAL_HOURS} —á–∞—Å–æ–≤")
        
        return deleted_count
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ë–î: {e}")
        return 0

# ---- Models ----
OUR_MODEL = None
YAMNET_MODEL = None
SCALER = None

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ scaler'–∞"""
    global OUR_MODEL, YAMNET_MODEL, SCALER
    
    try:
        # 1. –ù–æ–≤–∞—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (2079 –≤—Ö–æ–¥–æ–≤)
        OUR_MODEL = tf.keras.models.load_model(
            'cough_detection_improved_model.keras', 
            compile=False
        )
        logger.info("‚úÖ –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (2079 —Ñ–∏—á)")
        
        # 2. YAMNet
        YAMNET_MODEL = hub.load('https://tfhub.dev/google/yamnet/1')
        logger.info("‚úÖ YAMNet –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # 3. Scaler –∏–∑ –æ–±—É—á–µ–Ω–∏—è (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û!)
        SCALER = joblib.load('cough_scaler.pkl')
        logger.info("‚úÖ Scaler –∑–∞–≥—Ä—É–∂–µ–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        raise

# ---- Feature Extraction (–ù–û–í–´–ô –§–û–†–ú–ê–¢) ----
def extract_features_new(waveform, sr, yamnet_model):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç 2079 —Ñ–∏—á –∫–∞–∫ –≤ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        # 1. YAMNet embeddings
        waveform_tf = tf.convert_to_tensor(waveform, dtype=tf.float32)
        _, embeddings, _ = yamnet_model(waveform_tf)
        
        # –î–≤–∞ —Ç–∏–ø–∞ –ø—É–ª–ª–∏–Ω–≥–∞
        avg_pool = tf.reduce_mean(embeddings, axis=0).numpy()      # 1024
        max_pool = tf.reduce_max(embeddings, axis=0).numpy()       # 1024
        
        # 2. MFCC —Å mean –∏ std
        mfcc = librosa.feature.mfcc(
            y=waveform, sr=sr, n_mfcc=13, hop_length=512
        )
        mfcc_mean = np.mean(mfcc, axis=1)    # 13
        mfcc_std = np.std(mfcc, axis=1)      # 13
        
        # 3. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏
        spectral_centroid = librosa.feature.spectral_centroid(
            y=waveform, sr=sr, hop_length=512
        )[0]
        spectral_bandwidth = librosa.feature.spectral_bandwidth(
            y=waveform, sr=sr, hop_length=512
        )[0]
        
        spectral_features = np.array([
            np.mean(spectral_centroid),      # 1
            np.std(spectral_centroid),       # 1  
            np.mean(spectral_bandwidth)      # 1
        ])  # 3 —Ñ–∏—á–∏
        
        # 4. Zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(
            waveform, hop_length=512
        )[0]
        zcr_mean = np.mean(zcr)  # 1
        
        # 5. RMS —ç–Ω–µ—Ä–≥–∏–∏
        rms = librosa.feature.rms(y=waveform, hop_length=512)[0]
        rms_mean = np.mean(rms)  # 1
        
        # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ñ–∏—á–∏ (2079)
        combined = np.concatenate([
            avg_pool,           # 1024
            max_pool,           # 1024  
            mfcc_mean,          # 13
            mfcc_std,           # 13
            spectral_features,  # 3
            [zcr_mean, rms_mean]  # 2
        ])
        
        return combined
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á: {e}")
        return None

def analyze_audio(audio_bytes: bytes, filename: str) -> dict:
    """–ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ —Å –ù–û–í–û–ô –º–æ–¥–µ–ª—å—é"""
    if not OUR_MODEL or not SCALER or not YAMNET_MODEL:
        return {"probability": 0.0, "cough_detected": False, "message": "–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"}
    
    try:
        # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp:
            tmp.write(audio_bytes)
            tmp_path = tmp.name
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        waveform, sr = librosa.load(tmp_path, sr=16000, duration=1.0)
        os.unlink(tmp_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏—à–∏–Ω—É
        rms = float(np.sqrt(np.mean(waveform**2)))
        if rms < 0.01:
            return {"probability": 0.0, "cough_detected": False, "message": "–¢–∏—à–∏–Ω–∞"}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∫–∞–∫ –≤ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è)
        max_val = np.max(np.abs(waveform))
        if max_val < 0.01:
            return {"probability": 0.0, "cough_detected": False, "message": "–°–ª–∏—à–∫–æ–º —Ç–∏—Ö–æ"}
        
        waveform = waveform / (max_val + 1e-8)
        
        # –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ 1 —Å–µ–∫—É–Ω–¥—ã
        target_length = 16000
        if len(waveform) < target_length:
            waveform = np.pad(waveform, (0, target_length - len(waveform)))
        else:
            waveform = waveform[:target_length]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ù–û–í–´–• —Ñ–∏—á (2079)
        features = extract_features_new(waveform, sr, YAMNET_MODEL)
        if features is None:
            return {"probability": 0.0, "cough_detected": False, "message": "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏—á"}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ scaler (–í–ê–ñ–ù–û!)
        features_scaled = SCALER.transform(features.reshape(1, -1))
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = OUR_MODEL.predict(features_scaled, verbose=0)
        prob = float(prediction[0][0])
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        is_cough = prob > THRESHOLD
        
        logger.info(f"üéØ –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨: {filename} | prob={prob:.3f} | cough={is_cough}")
        
        return {
            "probability": prob,
            "cough_detected": bool(is_cough),
            "confidence": prob,
            "message": "COUGH_DETECTED" if is_cough else "NO_COUGH",
            "cough_count": 1 if is_cough else 0
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return {"probability": 0.0, "cough_detected": False, "message": f"–û—à–∏–±–∫–∞: {str(e)}"}

# ---- API Endpoints ----

@app.post("/upload")
async def upload_audio(audio: UploadFile = File(...), device_id: str = Form("unknown")):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ –∏ –∞–Ω–∞–ª–∏–∑"""
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞: {audio.filename}, device_id: {device_id}")
    
    try:
        # –ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø—Ä–∏ –∫–∞–∂–¥–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
        cleanup_old_records()
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        raw = await audio.read()
        if len(raw) == 0:
            raise HTTPException(400, "–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª")
        
        current_datetime = get_current_datetime()
        
        # –ê–Ω–∞–ª–∏–∑
        result = analyze_audio(raw, audio.filename)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO cough_records 
            (device_id, filename, file_path, probability, cough_detected, message, top_classes, cough_stats, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            device_id, 
            audio.filename,
            "",  # file_path –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º
            float(result["probability"]),
            int(result["cough_detected"]),
            result["message"],
            "[]",  # top_classes
            "{}",  # cough_stats
            current_datetime
        ))
        conn.commit()
        conn.close()
        
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        return JSONResponse({"status": "success", **result})
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        raise HTTPException(500, str(e))

@app.get("/stats/{device_id}")
async def get_stats(device_id: str):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ —Å–µ–≥–æ–¥–Ω—è (–æ—Å–Ω–æ–≤–Ω–æ–π endpoint)"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        today = get_current_date()
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute('''
            SELECT 
                COUNT(*) as total_recordings,
                SUM(cough_detected) as total_coughs,
                AVG(CASE WHEN cough_detected=1 THEN probability ELSE NULL END) as avg_probability
            FROM cough_records 
            WHERE device_id=? AND DATE(timestamp)=?
        ''', (device_id, today))
        
        stats = cursor.fetchone()
        total_recordings = int(stats[0] or 0) if stats else 0
        total_coughs = int(stats[1] or 0) if stats else 0
        avg_probability = float(stats[2] or 0.0) if stats and stats[2] is not None else 0.0
        
        # –ü–æ—á–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        hourly_stats = []
        for hour in range(24):
            hour_str = f"{hour:02d}:00"
            cursor.execute('''
                SELECT COUNT(*) FROM cough_records
                WHERE device_id=? AND cough_detected=1 AND DATE(timestamp)=? 
                AND strftime('%H', timestamp)=?
            ''', (device_id, today, f"{hour:02d}"))
            count_row = cursor.fetchone()
            count = int(count_row[0] or 0) if count_row else 0
            hourly_stats.append({"hour": hour_str, "count": count})
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∫–∞—à–ª–∏
        cursor.execute('''
            SELECT timestamp, probability FROM cough_records
            WHERE device_id=? AND cough_detected=1
            ORDER BY timestamp DESC LIMIT 10
        ''', (device_id,))
        recent_coughs = [
            {"time": row[0], "probability": float(row[1])} 
            for row in cursor.fetchall()
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã
        peak_hours = "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
        cough_frequency = "0 —Ä–∞–∑/–¥–µ–Ω—å"
        
        if total_coughs > 0:
            if hourly_stats:
                max_hour = max(hourly_stats, key=lambda x: x["count"])
                peak_hours = f"{max_hour['hour']} ({max_hour['count']} —Ä–∞–∑)"
            cough_frequency = f"{total_coughs} —Ä–∞–∑/–¥–µ–Ω—å"
        
        conn.close()
        
        result = {
            "today_stats": {
                "total_recordings": total_recordings,
                "total_coughs": total_coughs,
                "avg_probability": round(avg_probability, 3)
            },
            "hourly_stats": hourly_stats,
            "recent_coughs": recent_coughs,
            "patterns": {
                "peak_hours": peak_hours,
                "cough_frequency": cough_frequency,
                "intensity": "–í—ã—Å–æ–∫–∞—è" if avg_probability > 0.7 else "–°—Ä–µ–¥–Ω—è—è" if avg_probability > 0.3 else "–ù–∏–∑–∫–∞—è",
                "trend": "üìä"
            }
        }
        
        return result
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=500
        )

from fastapi.responses import RedirectResponse

@app.get("/debug/stats/{device_id}")
async def debug_stats_redirect(device_id: str):
    """–†–µ–¥–∏—Ä–µ–∫—Ç —Å–æ —Å—Ç–∞—Ä–æ–≥–æ endpoint'–∞ –Ω–∞ –Ω–æ–≤—ã–π"""
    return RedirectResponse(url=f"/stats/{device_id}")

@app.get("/records/all")
async def get_all_records(
    device_id: str = None, 
    limit: int = 100,
    offset: int = 0,
    include_audio: bool = False
):
    """–ü–æ–ª—É—á–∏—Ç—å –í–°–ï –∑–∞–ø–∏—Å–∏ (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)"""
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row  # –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –ø–æ –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–æ–∫
        cursor = conn.cursor()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if device_id:
            query = '''
                SELECT * FROM cough_records 
                WHERE device_id=? 
                ORDER BY timestamp DESC 
                LIMIT ? OFFSET ?
            '''
            params = (device_id, limit, offset)
            # –¢–∞–∫–∂–µ –ø–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —ç—Ç–æ–≥–æ device_id
            cursor.execute('SELECT COUNT(*) FROM cough_records WHERE device_id=?', (device_id,))
        else:
            query = '''
                SELECT * FROM cough_records 
                ORDER BY timestamp DESC 
                LIMIT ? OFFSET ?
            '''
            params = (limit, offset)
            # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π
            cursor.execute('SELECT COUNT(*) FROM cough_records')
        
        total_count = cursor.fetchone()[0]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
        cursor.execute(query, params)
        rows = cursor.fetchall()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
        records = []
        for row in rows:
            record = dict(row)
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã
            record['cough_detected'] = bool(record['cough_detected'])
            record['probability'] = float(record['probability'])
            
            # –ï—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω—ã –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ, —É–±–∏—Ä–∞–µ–º file_path
            if not include_audio:
                record.pop('file_path', None)
            
            records.append(record)
        
        conn.close()
        
        return {
            "status": "success",
            "total_records": total_count,
            "returned_records": len(records),
            "limit": limit,
            "offset": offset,
            "device_id": device_id,
            "records": records
        }
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–ø–∏—Å–µ–π: {e}")
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=500
        )

@app.get("/records/{device_id}/count")
async def get_records_count(device_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        cursor.execute('SELECT COUNT(*) FROM cough_records WHERE device_id=?', (device_id,))
        total = cursor.fetchone()[0]
        
        # –ö–∞—à–ª–∏ —Å–µ–≥–æ–¥–Ω—è
        today = get_current_date()
        cursor.execute('''
            SELECT COUNT(*) FROM cough_records 
            WHERE device_id=? AND cough_detected=1 AND DATE(timestamp)=?
        ''', (device_id, today))
        today_coughs = cursor.fetchone()[0]
        
        # –í—Å–µ –∫–∞—à–ª–∏
        cursor.execute('SELECT COUNT(*) FROM cough_records WHERE device_id=? AND cough_detected=1', (device_id,))
        all_coughs = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            "device_id": device_id,
            "total_records": total,
            "coughs_today": today_coughs,
            "total_coughs": all_coughs,
            "last_cleanup": CLEANUP_INTERVAL_HOURS
        }
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞: {e}")
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=500
        )

@app.delete("/records/cleanup")
async def manual_cleanup():
    """–†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
    try:
        deleted_count = cleanup_old_records()
        return {
            "status": "success",
            "message": f"–£–¥–∞–ª–µ–Ω–æ {deleted_count} –∑–∞–ø–∏—Å–µ–π —Å—Ç–∞—Ä—à–µ {CLEANUP_INTERVAL_HOURS} —á–∞—Å–æ–≤",
            "deleted_count": deleted_count
        }
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —Ä—É—á–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏: {e}")
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=500
        )

@app.get("/records/export/{device_id}")
async def export_records(device_id: str, format: str = "json"):
    """–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–ø–∏—Å–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT * FROM cough_records 
            WHERE device_id=? 
            ORDER BY timestamp
        ''', (device_id,))
        
        rows = cursor.fetchall()
        columns = [description[0] for description in cursor.description]
        
        conn.close()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä–∏
        records = []
        for row in rows:
            record = dict(zip(columns, row))
            record['cough_detected'] = bool(record['cough_detected'])
            records.append(record)
        
        if format.lower() == "csv":
            import csv
            import io
            
            output = io.StringIO()
            writer = csv.DictWriter(output, fieldnames=columns)
            writer.writeheader()
            writer.writerows(records)
            
            return {
                "status": "success",
                "format": "csv",
                "count": len(records),
                "data": output.getvalue()
            }
        
        else:  # json –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return {
                "status": "success",
                "format": "json",
                "count": len(records),
                "records": records
            }
        
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
        return JSONResponse(
            {"status": "error", "message": str(e)},
            status_code=500
        )

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞"""
    model_loaded = OUR_MODEL is not None and SCALER is not None
    db_exists = os.path.exists(DB_PATH)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ë–î
    db_status = "healthy"
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM cough_records')
        db_status = "healthy"
        conn.close()
    except:
        db_status = "unhealthy"
    
    return JSONResponse({
        "status": "healthy" if model_loaded and db_status == "healthy" else "degraded",
        "model_loaded": model_loaded,
        "scaler_loaded": SCALER is not None,
        "database": db_status,
        "database_path": DB_PATH,
        "cleanup_interval_hours": CLEANUP_INTERVAL_HOURS,
        "threshold": THRESHOLD,
        "timestamp": datetime.now().isoformat(),
        "features_dimension": 2079 if model_loaded else "unknown"
    })

@app.get("/")
async def root():
    return {
        "message": "üî• –£–õ–£–ß–®–ï–ù–ù–´–ô –°–µ—Ä–≤–µ—Ä –î–µ—Ç–µ–∫—Ü–∏–∏ –ö–∞—à–ª—è",
        "version": "2.0",
        "features": "–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å (2079 —Ñ–∏—á), –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞, —ç–∫—Å–ø–æ—Ä—Ç –∑–∞–ø–∏—Å–µ–π",
        "endpoints": {
            "POST /upload": "–ó–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            "GET /stats/{device_id}": "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ —Å–µ–≥–æ–¥–Ω—è",
            "GET /records/all": "–í—Å–µ –∑–∞–ø–∏—Å–∏ (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π)",
            "GET /records/{device_id}/count": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π",
            "GET /records/export/{device_id}": "–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–ø–∏—Å–µ–π",
            "DELETE /records/cleanup": "–†—É—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞",
            "GET /health": "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è"
        }
    }

# ---- Startup ----
@app.on_event("startup")
async def startup_event():
    """–ó–∞–ø—É—Å–∫ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∫–∞—à–ª—è...")
    init_db()
    load_models()
    logger.info(f"‚úÖ –°–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤! –ü–æ—Ä–æ–≥ –∫–∞—à–ª—è: {THRESHOLD}")
    logger.info(f"üßπ –ê–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞ –∫–∞–∂–¥—ã–µ {CLEANUP_INTERVAL_HOURS} —á–∞—Å–æ–≤")

# ---- Main ----
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"üöÄ Starting IMPROVED COUGH SERVER on port {port}")
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")

